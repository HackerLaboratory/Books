网络爬虫从流程上来说很简单：

* 下载HTML页面
* 解析HTML页面
* 管理URL
* 存储解析结果

但往细处深究，总是有很多技术性的细节需要花大功夫钻研，有很多技术难题需要花时间解决：

* 代理服务器过期的话怎么办
* 遇到反爬虫机制，比如要求输入验证码怎么办
* 如何制定爬虫算法高效、完整的爬取网页
* 如何使用正则表达式正确的解析网页
* 如何将网页缓存下来保证不重复下载，又能保证及时更新
* 快速爬取会对服务器造成压力、爬的太慢则效率太低，如何处理这个矛盾
* 单线程、单进程爬取太慢，如何利用多线程、多进程提升效率
* 爬取到的大量数据存储到数据库会有性能问题，如何进行数据库调优
* 等等

所以简单的流程背后还有着大量的技术难点，这也是高手之所以为高手的原因所在！

